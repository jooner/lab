{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beast1/.virtualenvs/lab/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pandas import read_csv\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Load and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tolstoy_anna.txt', 'r') as f:\n",
    "    txt = f.readlines()\n",
    "txt = [x.strip() for x in txt]\n",
    "txt_str = ''\n",
    "for t in txt:\n",
    "    txt_str += ' {}'.format(t.lower())\n",
    "tokens = word_tokenize(txt_str)\n",
    "# word2idx and idx2word setup\n",
    "unique_tokens = set(tokens)\n",
    "w2x = {word: idx for (idx, word) in enumerate(unique_tokens)}\n",
    "x2w = {idx: word for (idx, word) in enumerate(unique_tokens)}\n",
    "indices = [w2x[w] for w in tokens]\n",
    "vocab_size = len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous Bag-of-Words Model\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim,\n",
    "                 context_size, batch_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.out = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x).view(self.batch_size, 1, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.out(x).squeeze()\n",
    "\n",
    "model = CBOW(vocab_size=vocab_size, embedding_dim=100, hidden_dim=128,\n",
    "             context_size=2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = torch.load('models/model_14.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tolstoy_writes(start_word, length):\n",
    "    speech = \"{}\".format(start_word.title())\n",
    "    x = w2x[start_word]\n",
    "    capital = False\n",
    "    for _ in range(length):\n",
    "        p_dist = model(Variable(torch.LongTensor([x])))\n",
    "        x = int(D.Categorical(p_dist).sample())\n",
    "        w = x2w[x]\n",
    "        if capital:\n",
    "            speech += \" {}\".format(w.title())\n",
    "        else:\n",
    "            speech += \" {}\".format(w)\n",
    "        if \".\" in w:\n",
    "            capital = True\n",
    "        else:\n",
    "            capital = False\n",
    "    print(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tolstoy_writes('love', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls choco_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chocolate Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training data\n",
    "window = 5\n",
    "past_data, future_data, target_data = [], [], []\n",
    "for idx in range(len(indices)):\n",
    "    past, future = [], []\n",
    "    for r in range(-window, window + 1):\n",
    "        cxt = idx + r\n",
    "        if (r < 0) and not ((cxt < 0) or (cxt >= len(indices))):\n",
    "            past.append(indices[cxt])\n",
    "        elif (r > 0) and not ((cxt < 0) or (cxt >= len(indices))):\n",
    "            future.append(indices[cxt])\n",
    "    if len(past) == len(future) == window:\n",
    "        past_data.append(past)\n",
    "        future_data.append(future)\n",
    "        target_data.append(indices[idx])\n",
    "past_data = torch.LongTensor(past_data)\n",
    "future_data = torch.LongTensor(future_data)\n",
    "target_data = torch.LongTensor(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(idx_batch):\n",
    "    one_hot_mat = torch.zeros((len(idx_batch), vocab_size)).float()\n",
    "    indices = torch.LongTensor(idx_batch).view(-1, 1)\n",
    "    one_hot_mat.scatter_(1, indices, 1.0)\n",
    "    return one_hot_mat\n",
    "\n",
    "def mat_loss(pred, gt):\n",
    "    delta = pred - gt\n",
    "    norm = torch.norm(delta, p=2, dim=1)\n",
    "    return torch.log(torch.sum(norm) / gt.shape[1])\n",
    "\n",
    "def batchify(data, batch_size, use_cuda=False):\n",
    "    rm_size = len(data) % batch_size\n",
    "    data = data[:-rm_size].contiguous()\n",
    "    if len(data.shape) == 1:\n",
    "        data = data.view(-1, batch_size)\n",
    "    else:\n",
    "        data = data.view(-1, batch_size,\n",
    "                         *data.shape[1:])\n",
    "    if use_cuda:\n",
    "        return data.cuda()\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passe-Avenir co-predict predictor\n",
    "class PasseAvenir(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim,\n",
    "                 window, batch_size, embed=None):\n",
    "        super(PasseAvenir, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.emb_dim = embedding_dim\n",
    "        self.window = window\n",
    "        if not embed:\n",
    "            self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        else:\n",
    "            self.embed = embed\n",
    "        self.lstm = nn.LSTM(self.emb_dim, hidden_dim, 2, dropout=0.2)\n",
    "        self.fc = nn.Linear(window * hidden_dim, self.emb_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x).view(self.window, self.batch_size, self.emb_dim)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.view(self.batch_size, -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passe-Avenir manager\n",
    "class PA2Manager(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, batch_size=1):\n",
    "        super(PA2Manager, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.fc1 = nn.Linear(embed_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.out = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(self.batch_size, self.embed_dim * 2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_p, X_f, y, num_epochs, batch_size, use_cuda=False):\n",
    "    loss_fn = mat_loss\n",
    "    params = list(Past.parameters()) + list(Future.parameters()) + \\\n",
    "             list(Manager.parameters())\n",
    "    optimizer = optim.Adam(params, lr=1e-2)\n",
    "    losses = []\n",
    "    for epoch in tnrange(num_epochs, desc='epoch'):\n",
    "        total_loss = 0\n",
    "        # shuffle\n",
    "        comb = list(zip(X_p, X_f, y))\n",
    "        random.shuffle(comb)\n",
    "        X_p, X_f, y = zip(*comb)\n",
    "        X_p, X_f, y = torch.stack(X_p), torch.stack(X_f), torch.stack(y)\n",
    "        for batch_idx in tqdm_notebook(range(y.shape[0]),\n",
    "                                           desc='index', leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            x = torch.cat([Past(X_p[batch_idx]),\n",
    "                           Future(X_f[batch_idx])], dim=1)\n",
    "            log_prob = Manager(x)\n",
    "            gt = one_hot(y[batch_idx])\n",
    "            if use_cuda:\n",
    "                gt = gt.cuda()\n",
    "            loss = loss_fn(log_prob, gt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data\n",
    "        losses.append(total_loss)\n",
    "        l = float(total_loss / y.shape[0])\n",
    "        print(\"EPOCH: {}/{} | AVG LOG LOSS: {}\".format(epoch + 1, num_epochs, l))\n",
    "        if len(losses) >= 2:\n",
    "            print(\"LOSS CHANGE: {}%\".format(round(float(100 * (losses[-1] -  losses[-2]) / losses[-2]), 5)))\n",
    "        if epoch % 20 == 0:\n",
    "            torch.save(Past.state_dict(),\n",
    "                       'choco_models/past_{}_{}.pt'.format(epoch + 1, round(l, 4)))\n",
    "            torch.save(Future.state_dict(),\n",
    "                       'choco_models/future_{}_{}.pt'.format(epoch + 1, round(l, 4)))\n",
    "            torch.save(Manager.state_dict(),\n",
    "                       'choco_models/manager_{}_{}.pt'.format(epoch + 1, round(l, 4)))\n",
    "        print(\"Successfully saved model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "window = 5        #\n",
    "emb_dim = 100     #\n",
    "num_epochs = 1000 #\n",
    "batch_size = 64   #\n",
    "pa_hid_dim = 256  #\n",
    "mgr_hid_dim = 256 #\n",
    "###################\n",
    "\n",
    "Past = PasseAvenir(vocab_size=vocab_size, embedding_dim=emb_dim,\n",
    "                   hidden_dim=pa_hid_dim, window=window,\n",
    "                   batch_size=batch_size, embed=None).cuda() # enable cuda\n",
    "Future = PasseAvenir(vocab_size=vocab_size, embedding_dim=emb_dim,\n",
    "                     hidden_dim=pa_hid_dim, window=window,\n",
    "                     batch_size=batch_size, embed=None).cuda() # enable cuda\n",
    "Manager = PA2Manager(vocab_size=vocab_size, embed_dim=emb_dim,\n",
    "                     hidden_dim=mgr_hid_dim, batch_size=batch_size).cuda() # enable cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886755075bee4cc3b82a4ad02f621826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch', max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='index', max=6731), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1/1000 | AVG LOG LOSS: -5.489077568054199\n",
      "Successfully saved model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da9e2b6a2084ff1af2e97efaec849cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='index', max=6731), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_past = batchify(past_data, batch_size=batch_size, use_cuda=True) # enable cuda\n",
    "X_future = batchify(future_data, batch_size=batch_size, use_cuda=True) # enable cuda\n",
    "y = batchify(target_data, batch_size=batch_size, use_cuda=False)\n",
    "# train\n",
    "train(X_past, X_future, y, num_epochs=num_epochs, batch_size=batch_size, use_cuda=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
