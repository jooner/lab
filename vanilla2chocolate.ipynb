{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import PriorityQueue\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pandas import read_csv\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla Load and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tolstoy_anna.txt', 'r') as f:\n",
    "    txt = f.readlines()\n",
    "txt = [x.strip() for x in txt]\n",
    "txt_str = ''\n",
    "for t in txt:\n",
    "    txt_str += ' {}'.format(t.lower())\n",
    "tokens = word_tokenize(txt_str)\n",
    "# word2idx and idx2word setup\n",
    "unique_tokens = set(tokens)\n",
    "w2x = {word: idx for (idx, word) in enumerate(unique_tokens)}\n",
    "x2w = {idx: word for (idx, word) in enumerate(unique_tokens)}\n",
    "indices = [w2x[w] for w in tokens]\n",
    "vocab_size = len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous Bag-of-Words Model\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim,\n",
    "                 context_size, batch_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.out = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x).view(self.batch_size, 1, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.out(x).squeeze()\n",
    "\n",
    "model = CBOW(vocab_size=vocab_size, embedding_dim=100, hidden_dim=128,\n",
    "             context_size=2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = torch.load('models/model_9.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (embed): Embedding(15080, 100)\n",
       "  (fc1): Linear(in_features=100, out_features=128)\n",
       "  (fc2): Linear(in_features=128, out_features=15080)\n",
       "  (out): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tolstoy_writes(start_word, length):\n",
    "    speech = \"{}\".format(start_word.title())\n",
    "    x = w2x[start_word]\n",
    "    capital = False\n",
    "    for _ in range(length):\n",
    "        p_dist = model(Variable(torch.LongTensor([x])))\n",
    "        x = int(D.Categorical(p_dist).sample())\n",
    "        w = x2w[x]\n",
    "        if capital:\n",
    "            speech += \" {}\".format(w.title())\n",
    "        else:\n",
    "            speech += \" {}\".format(w)\n",
    "        if \".\" in w:\n",
    "            capital = True\n",
    "        else:\n",
    "            capital = False\n",
    "    print(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joy tire intelligence. Patched lawyer whim thee bezzubov lavished preparing trot nikandrov tips opponents keiss mangle infallible concluded petty committeesâ€”everywhere _tiutkin\n"
     ]
    }
   ],
   "source": [
    "tolstoy_writes('joy', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chocolate Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window = 2\n",
    "# X, y = [], []\n",
    "# for i, token in enumerate(tokens):\n",
    "#     tmp = []\n",
    "#     for w in range(-window, window + 1):\n",
    "#         if not (i + w < 0 or i + w >= len(tokens) or w == 0):\n",
    "#             tmp.append(w2x[tokens[i + w]])\n",
    "#     if len(tmp) == window * 2:\n",
    "#         X.append(tmp)\n",
    "#         y.append(w2x[token])\n",
    "# X = np.array(X)\n",
    "# y = np.array(y)\n",
    "# X = Variable(torch.LongTensor(X))\n",
    "# y = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(idx):\n",
    "    one_hot_mat = torch.zeros(vocab_size).float()\n",
    "    one_hot_mat[idx] = 1.0\n",
    "    return one_hot_mat\n",
    "\n",
    "\n",
    "def vec_loss(pred, gt):\n",
    "    delta = pred - Variable(gt)\n",
    "    return torch.sum(delta)\n",
    "\n",
    "\n",
    "def batchify(data, batch_size, use_cuda=False):\n",
    "    rm_size = len(data) % batch_size\n",
    "    x = data[:-rm_size, 0].contiguous()\n",
    "    y = data[:-rm_size, 1].contiguous()\n",
    "    if use_cuda:\n",
    "        x = x.view(-1, batch_size).cuda()\n",
    "    else:\n",
    "        x = x.view(-1, batch_size)\n",
    "    y = y.view(-1, batch_size)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passe-Avenir co-predict predictor\n",
    "class PasseAvenir2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim,\n",
    "                 window, batch_size, embed=None):\n",
    "        super(PasseAvenir2, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.emb_dim = embedding_dim\n",
    "        self.window = window\n",
    "        if not embed:\n",
    "            self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        else:\n",
    "            self.embed = embed\n",
    "        self.lstm = nn.LSTM(self.emb_dim, hidden_dim, 1, dropout=0)\n",
    "        self.fc1 = nn.Linear(window * hidden_dim, vocab_size)\n",
    "        self.out = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x).view(self.window, self.batch_size, self.emb_dim)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.view(self.batch_size, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.out(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PastOne = PasseAvenir2(vocab_size=vocab_size,\n",
    "                       embedding_dim=100, hidden_dim=128,\n",
    "                       window=3, batch_size=1, embed=model.embed)\n",
    "PastTwo = PasseAvenir2(vocab_size=vocab_size,\n",
    "                       embedding_dim=100, hidden_dim=128,\n",
    "                       window=3, batch_size=1, embed=model.embed)\n",
    "FutureOne = PasseAvenir2(vocab_size=vocab_size,\n",
    "                         embedding_dim=100, hidden_dim=128,\n",
    "                         window=3, batch_size=1, embed=model.embed)\n",
    "FutureTwo = PasseAvenir2(vocab_size=vocab_size,\n",
    "                         embedding_dim=100, hidden_dim=128,\n",
    "                         window=3, batch_size=1, embed=model.embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passe-Avenir manager\n",
    "class PA2Manager(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, window, batch_size=1):\n",
    "        super(PA2Manager, self).__init__()\n",
    "        self.window = window\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm = nn.LSTM(self.vocab_size, hidden_dim, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(window * 2 * hidden_dim * 2, vocab_size)\n",
    "        self.out = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.stack(x, dim=0)\n",
    "        x = x.view(self.window * 2, self.batch_size, self.vocab_size)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.view(self.batch_size, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.out(x).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Manager = PA2Manager(vocab_size=vocab_size, hidden_dim=128, window=2, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(indices, num_epochs, use_cuda=False):\n",
    "    loss_fn = vec_loss\n",
    "    params = list(PastOne.parameters()) + list(PastTwo.parameters()) + \\\n",
    "             list(FutureOne.parameters()) + list(FutureTwo.parameters()) + \\\n",
    "             list(Manager.parameters())\n",
    "    optimizer = optim.SGD(params, lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.9)\n",
    "    indices = torch.LongTensor(indices)\n",
    "    for epoch in tnrange(num_epochs, desc='epoch'):\n",
    "        total_loss = 0\n",
    "        # batch_size 1 only for now\n",
    "        for idx in tqdm_notebook(range(5, indices.shape[0] - 5),\n",
    "                                       desc='index', leave=False):\n",
    "            model.zero_grad()\n",
    "            ############################\n",
    "            # TODO: We can Batch Here! # \n",
    "            ############################\n",
    "            \n",
    "            # window size is 3 for these\n",
    "            f1 = FutureOne(Variable(indices[idx - 3 : idx]))\n",
    "            f2 = FutureTwo(Variable(indices[idx - 4: idx - 1]))\n",
    "            p1 = PastOne(Variable(indices[idx + 1 : idx + 4]))\n",
    "            p2 = PastTwo(Variable(indices[idx + 2 : idx + 5]))\n",
    "            x = [f2, f1, p1, p2]\n",
    "            log_prob = Manager(x)\n",
    "            gt = one_hot(indices[idx])\n",
    "            loss = loss_fn(log_prob, gt)\n",
    "            loss.backward()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.data\n",
    "            if idx % 2000 == 0:\n",
    "                num_seen = (idx + 1)\n",
    "                l = np.log(float(total_loss / num_seen))\n",
    "                print(\"BATCH: {}/{} | AVG LOG LOSS: {}\".format(idx + 1,\n",
    "                                                           len(indices),\n",
    "                                                           l))\n",
    "            if idx % (len(indices) // 4) == 0:\n",
    "                torch.save(model.state_dict(), 'choco_models/model_{}.pt'.format(epoch))\n",
    "                print(\"Successfully saved model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(indices, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
